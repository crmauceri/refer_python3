{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from refer import REFER\n",
    "import numpy as np\n",
    "import sys\n",
    "import os.path as osp\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset sunspot into memory...\n",
      "creating index...\n",
      "index created.\n",
      "DONE (t=0.60s)\n"
     ]
    }
   ],
   "source": [
    "data_root = './data'  # contains refclef, refcoco, refcoco+, refcocog and images\n",
    "dataset = 'sunspot'\n",
    "splitBy = 'boulder'\n",
    "refer = REFER(data_root, dataset, splitBy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Evaluate Refering Expressions by Language Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, './evaluation')\n",
    "from refEvaluation import RefEvaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sent': 'man in black', 'ref_id': 47}\n"
     ]
    }
   ],
   "source": [
    "# Here's our example expression file\n",
    "sample_expr_file = json.load(open('test/sample_expressions_testA.json', 'r'))\n",
    "sample_exprs = sample_expr_file['predictions']\n",
    "print(sample_exprs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "110",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-9f74162c233f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrefEval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRefEvaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrefer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_exprs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrefEval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Workspace/refer_python3/evaluation/refEvaluation.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mrefToGts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mref_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mevalRefIds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRefs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mref_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mgt_sents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sent'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentences'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# up to 3 expressions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mrefToGts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mref_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgt_sents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 110"
     ]
    }
   ],
   "source": [
    "refEval = RefEvaluation(refer, sample_exprs)\n",
    "refEval.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Evaluate Referring Expressions by Duplicate Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evalue how many images contain duplicate expressions\n",
    "pred_refToSent = {int(it['ref_id']): it['sent'] for it in sample_exprs}\n",
    "pred_imgToSents = {}\n",
    "for ref_id, pred_sent in pred_refToSent.items():\n",
    "    image_id = refer.Refs[ref_id]['image_id']\n",
    "    pred_imgToSents[image_id] = pred_imgToSents.get(image_id, []) + [pred_sent]\n",
    "# count duplicate\n",
    "duplicate = 0\n",
    "for image_id, sents in pred_imgToSents.items():\n",
    "    if len(set(sents)) < len(sents):\n",
    "        duplicate += 1\n",
    "ratio = duplicate*100.0 / len(pred_imgToSents)\n",
    "print '%s/%s (%.2f%%) images have duplicate predicted sentences.' % (duplicate, len(pred_imgToSents), ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 3.Evaluate Referring Comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IoU function\n",
    "def computeIoU(box1, box2):\n",
    "    # each box is of [x1, y1, w, h]\n",
    "    inter_x1 = max(box1[0], box2[0])\n",
    "    inter_y1 = max(box1[1], box2[1])\n",
    "    inter_x2 = min(box1[0]+box1[2]-1, box2[0]+box2[2]-1)\n",
    "    inter_y2 = min(box1[1]+box1[3]-1, box2[1]+box2[3]-1)\n",
    "\n",
    "    if inter_x1 < inter_x2 and inter_y1 < inter_y2:\n",
    "        inter = (inter_x2-inter_x1+1)*(inter_y2-inter_y1+1)\n",
    "    else:\n",
    "        inter = 0\n",
    "    union = box1[2]*box1[3] + box2[2]*box2[3] - inter\n",
    "    return float(inter)/union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly sample one ref\n",
    "ref_ids = refer.getRefIds()\n",
    "ref_id = ref_ids[np.random.randint(0, len(ref_ids))]\n",
    "ref = refer.Refs[ref_id]\n",
    "\n",
    "# let's fake one bounding box by randomly picking one instance inside this image\n",
    "image_id = ref['image_id']\n",
    "anns = refer.imgToAnns[image_id]\n",
    "ann = anns[np.random.randint(0, len(anns))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw box of the ref using 'green'\n",
    "plt.figure()\n",
    "refer.showRef(ref, seg_box='box')\n",
    "# draw box of the ann using 'red'\n",
    "ax = plt.gca()\n",
    "bbox = ann['bbox']\n",
    "box_plot = Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3], fill=False, edgecolor='red', linewidth=2)\n",
    "ax.add_patch(box_plot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is the ann actually our ref?\n",
    "# i.e., IoU >= 0.5?\n",
    "ref_box = refer.refToAnn[ref_id]['bbox']\n",
    "ann_box = ann['bbox']\n",
    "IoU = computeIoU(ref_box, ann_box)\n",
    "if IoU >= 0.5:\n",
    "    print 'IoU=[%.2f], correct comprehension!' % IoU\n",
    "else:\n",
    "    print 'IoU=[%.2f], wrong comprehension!' % IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
